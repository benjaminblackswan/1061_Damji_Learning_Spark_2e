# Databricks notebook source
# Define a schema using DDL method pg 51
schema = "Id INT, First STRING, Last STRING, Url STRING, Published STRING, Hits INT, Campaigns ARRAY<STRING>"

# COMMAND ----------

# creating static data pg 52
data = [
[1, "Jules", "Damji", "https://tinyurl.1", "1/4/2016", 4535, ["twitter", "LinkedIn"]],
[2, "Brooke","Wenig", "https://tinyurl.2", "5/5/2018", 8908, ["twitter", "LinkedIn"]],
[3, "Denny", "Lee", "https://tinyurl.3", "6/7/2019", 7659, ["web", "twitter", "FB", "LinkedIn"]],
[4, "Tathagata", "Das", "https://tinyurl.4", "5/12/2018", 10568, ["twitter", "FB"]],
[5, "Matei","Zaharia", "https://tinyurl.5", "5/14/2014", 40578, ["web", "twitter", "FB", "LinkedIn"]],
[6, "Reynold", "Xin", "https://tinyurl.6", "3/2/2015", 25568, ["twitter", "LinkedIn"]]
]

# COMMAND ----------

# page 52
# Create a DataFrame using the schema and static data defined above
blogs_df = spark.createDataFrame(data, schema)

# Show the DataFrame; it should reflect our table above
blogs_df.show()
# Print the schema used by Spark to process the DataFrame
print(blogs_df.printSchema())

# COMMAND ----------

# page 53, If you want to use this schema elsewhere in your code, simply execute
blogs_df.schema

# COMMAND ----------

# pg 57, using Row object to instantiate a row
from pyspark.sql import *
blog_row = Row(6, "Reynold", "Xin", "https://tinyurl.6", "3/2/2015", 25568, ["twitter", "LinkedIn"])
blog_row[1]

# COMMAND ----------

# pg 57, using Row object to create DataFrames for quick exploration
rows = [Row("Matei Zaharia", "CA"), Row("Reynold Xin", "CA")]
authors_df = spark.createDataFrame(rows, ["Authors", "State"])
authors_df.show()

# COMMAND ----------

# pg 59, create data DataFrame for San Francisco Fire Department calls

# We must first import types because we will use StructType
from pyspark.sql.types import *

fire_schema = StructType([StructField('CallNumber', IntegerType(), True),
StructField('UnitID', StringType(), True),
StructField('IncidentNumber', IntegerType(), True),
StructField('CallType', StringType(), True),
StructField('CallDate', StringType(), True),
StructField('WatchDate', StringType(), True),
StructField('CallFinalDisposition', StringType(), True),
StructField('AvailableDtTm', StringType(), True),
StructField('Address', StringType(), True),
StructField('City', StringType(), True),
StructField('Zipcode', IntegerType(), True),
StructField('Battalion', StringType(), True),
StructField('StationArea', StringType(), True),
StructField('Box', StringType(), True),
StructField('OriginalPriority', StringType(), True),
StructField('Priority', StringType(), True),
StructField('FinalPriority', IntegerType(), True),
StructField('ALSUnit', BooleanType(), True),
StructField('CallTypeGroup', StringType(), True),
StructField('NumAlarms', IntegerType(), True),
StructField('UnitType', StringType(), True),
StructField('UnitSequenceInCallDispatch', IntegerType(), True),
StructField('FirePreventionDistrict', StringType(), True),
StructField('SupervisorDistrict', StringType(), True),
StructField('Neighborhood', StringType(), True),
StructField('Location', StringType(), True),
StructField('RowID', StringType(), True),
StructField('Delay', FloatType(), True)])

# Use the DataFrameReader interface to read a CSV file
sf_fire_file = "/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv"
fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)

display(fire_df)

# COMMAND ----------

# import all functions, as col() is a function
from pyspark.sql.functions import *

# pg 61, filtering dataset
few_fire_df = (fire_df
               .select("IncidentNumber", "AvailableDtTm", "CallType")
               .where(col("CallType") != "Medical Incident"))

# to show the first 5 returns and NOT truncate any columns
few_fire_df.show(5, truncate=False)

# COMMAND ----------

# pg 62, return number of distinct types of calls
from pyspark.sql.functions import *

fire_df.select("CallType") \
        .where(col("CallType").isNotNull()) \
        .agg(countDistinct("CallType").alias("DistinctCallTypes")) \
        .show()


# another way of writing the code is to wrap everything in (), this way, no need to add line breaks "\" at the end of each line.

(
    fire_df.select("CallType")
        .where(col("CallType").isNotNull())
        .agg(countDistinct("CallType").alias("DistinctCallTypes"))
        .show()
)

# COMMAND ----------

(
    fire_df
        .select("CallType")
        .where(col("CallType").isNotNull())
        .distinct()
        .show(50, False)
)

# COMMAND ----------

# renaming a column in dataFrame
new_fire_df = fire_df.withColumnRenamed("Delay", "ResponseDelayinMins")

# select data with responseDelay time greater than 5
(
    new_fire_df
        .select("ResponseDelayinMins")
        .where(col("ResponseDelayinMins") > 5)
        .show(5, False)
)

# COMMAND ----------

# pg 64, change string to date formats
fire_ts_df = (new_fire_df
              .withColumn("IncidentDate", to_timestamp(col("CallDate"), "MM/dd/yyyy"))
              .drop("CallDate")
              .withColumn("OnWatchDate", to_timestamp(col("WatchDate"), "MM/dd/yyyy"))
              .drop("WatchDate")
              .withColumn("AvailableDtTS", to_timestamp(col("AvailableDtTm"),
              "MM/dd/yyyy hh:mm:ss a"))
              .drop("AvailableDtTm"))

(fire_ts_df
    .select("IncidentDate", "OnWatchDate", "AvailableDtTS")
    .show(5, False)
 )

# COMMAND ----------

# now we can query using date and time functions such as month() etc

(
    fire_ts_df
        .select(year('IncidentDate'))
        .distinct()
        .orderBy(year('IncidentDate'))
        .show()
)

# COMMAND ----------

# using group by

(
    fire_ts_df
      .select("CallType")
      .where(col("CallType").isNotNull())
      .groupBy("CallType")
      .count()
      .orderBy("count", ascending=False)
      .show(n = 10, truncate = False)
)

# COMMAND ----------

import pyspark.sql.functions as F
(fire_ts_df
    .select(F.sum("NumAlarms"), F.avg("ResponseDelayinMins"),
            F.min("ResponseDelayinMins"), F.max("ResponseDelayinMins"))
    .show()
)

# COMMAND ----------

from pyspark.sql import Row
row = Row(350, True, "Learning Spark 2E", None)

# COMMAND ----------

row[2]
